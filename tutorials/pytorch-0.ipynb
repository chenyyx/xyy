{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch 教程\n",
    "## 1、pytorch 基础\n",
    "这里就不再介绍 pytorch 的安装了，开始我们的教程的第一部分，介绍 pytorch 处理的对象以及操作。\n",
    "\n",
    "### 1.1、Tensor\n",
    "pytorch 里面最基本的操作对象，tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9618e-43,  1.5554e-43,  1.5975e-43,  1.3873e-43],\n",
       "        [ 1.4574e-43,  6.4460e-44,  1.1771e-43,  1.4153e-43],\n",
       "        [ 1.5414e-43,  1.6115e-43,  1.5554e-43,  1.5975e-43],\n",
       "        [ 5.6052e-44,  7.4269e-44,  6.1657e-44,  7.2868e-44],\n",
       "        [ 5.7453e-44,  7.3313e+22,  1.8471e+25,  4.1216e+21]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的数组大小 5x4 的矩阵\n",
    "torch.Tensor(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7319,  0.9140,  0.7606,  0.7159],\n",
       "        [ 0.2685,  0.6484,  0.7076,  0.4852],\n",
       "        [ 0.0037,  0.0202,  0.0826,  0.2507],\n",
       "        [ 0.4765,  0.9968,  0.0194,  0.8681],\n",
       "        [ 0.2938,  0.5170,  0.6764,  0.0367]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的数组大小是 5x4 的矩阵，初始化是 0~1 的均匀分布\n",
    "torch.rand(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到矩阵大小\n",
    "a = torch.rand(5,4)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 类似的返回 5x4 大小的矩阵\n",
    "np.ones((5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6287986   0.90508151  0.11585796  0.25815994]\n",
      " [ 0.7688539   0.05344862  0.67173809  0.38229114]\n",
      " [ 0.94842637  0.77569878  0.95060408  0.36145872]\n",
      " [ 0.22757459  0.2761066   0.56519628  0.70383507]\n",
      " [ 0.10005569  0.31175494  0.93085307  0.05551481]]\n"
     ]
    }
   ],
   "source": [
    "# numpy 和 torc.Tensor 之间的转换\n",
    "a = torch.rand(5,4)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  3],\n",
      "        [ 4,  5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2,3],[4,5]])\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运算和 numpy 类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5,4)\n",
    "y = torch.rand(5,4)\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4698,  0.5713,  0.5707,  1.9875],\n",
      "        [ 2.8717,  2.4169,  2.7111,  1.7490],\n",
      "        [ 2.0731,  2.4739,  0.8732,  2.8371],\n",
      "        [ 2.7190,  2.2233,  2.7300,  2.9132],\n",
      "        [ 2.6850,  0.0662,  1.7391,  1.9291]])\n"
     ]
    }
   ],
   "source": [
    "print(c * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5905,  0.5386,  0.3262,  0.7107],\n",
      "        [ 1.2793,  1.1548,  1.1620,  0.9617],\n",
      "        [ 0.6923,  1.3231,  0.7628,  1.8557],\n",
      "        [ 1.5553,  1.6721,  1.4354,  1.1975],\n",
      "        [ 1.0541,  0.9008,  1.0414,  0.7672]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5905,  0.5386,  0.3262,  0.7107],\n",
      "        [ 1.2793,  1.1548,  1.1620,  0.9617],\n",
      "        [ 0.6923,  1.3231,  0.7628,  1.8557],\n",
      "        [ 1.5553,  1.6721,  1.4354,  1.1975],\n",
      "        [ 1.0541,  0.9008,  1.0414,  0.7672]])\n"
     ]
    }
   ],
   "source": [
    "print(x.add(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6910,  0.8867,  0.4621,  0.7589],\n",
       "        [ 1.6014,  1.5041,  1.4204,  1.3404],\n",
       "        [ 0.6936,  1.8216,  1.2345,  2.7658],\n",
       "        [ 2.2043,  2.6030,  1.9609,  1.4240],\n",
       "        [ 1.2132,  1.7794,  1.5032,  0.8913]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以直接进行操作改变原对象，x+y 或者 x.add() 并不会改变x，但是 x.add_() 则会对 x 进行改变\n",
    "x.add_(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6910,  0.8867,  0.4621,  0.7589],\n",
      "        [ 1.6014,  1.5041,  1.4204,  1.3404],\n",
      "        [ 0.6936,  1.8216,  1.2345,  2.7658],\n",
      "        [ 2.2043,  2.6030,  1.9609,  1.4240],\n",
      "        [ 1.2132,  1.7794,  1.5032,  0.8913]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 torch.Tensor 放到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断一下电脑是否支持 GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2、torch 的自动求导功能\n",
    "torch 和大部分框架一样有着自动求导功能，对象不再是 torch.Tensor，而是 torch.autograd.Variable  \n",
    "本质上 Variable 和 Tensor 没有什么区别，不过 Variable 会放在一个计算图里面，可以进行前向传播和反向传播以及求导  \n",
    "里面的 creator 表示通过什么操作得到的这个 Variable ，grad 表示反向传播的梯度  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# requires_grad 表示是否对其求梯度，默认是 False\n",
    "x = Variable(torch.Tensor([3]),requires_grad=True)\n",
    "y = Variable(torch.Tensor([5]),requires_grad=True)\n",
    "z = 2 * x + y + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对 x 和 y 分别求导\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor([ 2.])\n",
      "dz/dy: tensor([ 1.])\n"
     ]
    }
   ],
   "source": [
    "# x 的导数和 y 的导数\n",
    "print('dz/dx: {}'.format(x.grad.data))\n",
    "print('dz/dy: {}'.format(y.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3、神经网络部分\n",
    "所依赖的主要是 torch.nn 和 torch.nn.functional  \n",
    "torch.nn 里面有着所有的神经网络的层的操作，其用来构建网络，只有执行一次网络的运算才执行一次  \n",
    "torch.nn.functional 表示的是直接对其做一次向前运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基本的网络构建类模板\n",
    "class net_name(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_name, self).__init__()\n",
    "        # 可以添加各种网络层\n",
    "        self.conv1 = nn.Conv2d(3, 10 ,3)\n",
    "        # 具体每种层的参数可以去查看文档\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播\n",
    "        out = self.conv1(x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
