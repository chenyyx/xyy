{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "\n",
    "## 1、前言\n",
    "\n",
    "经过前面几节的学习，终于完成了我们的基础部分，下面正式进入到了深度学习部分。\n",
    "\n",
    "第一个要讲的当然是 cnn 了，也就是卷积神经网络，具体的就不再仔细介绍了，我们直接进入代码部分。\n",
    "\n",
    "## 2、模型\n",
    "\n",
    "数据集仍然是使用 MNIST 手写字体，和之前一样做同样的预处理。\n",
    "\n",
    "### 2.1、导入库和包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2、定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "num_epoches = 5\n",
    "\n",
    "# 定义 Convolution Network 模型\n",
    "class Cnn(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 6, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Linear(84, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = Cnn(1, 10)  # 图片大小是28x28\n",
    "use_gpu = torch.cuda.is_available()  # 判断是否有GPU加速\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "# 定义loss和optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是网络的模型部分了。和之前相比主要增加了这些不一样的部分。\n",
    "\n",
    " - 1、 nn.Sequential()\n",
    " 这个表示将一个有序的模块写在一起，也就相当于将神经网络的层按顺序放在一起，这样可以方便结构显示。\n",
    " - 2、nn.Conv2d()\n",
    " 这个是卷积层，里面常用的参数有四个，in_channels,out_channels,kernel_size,stride, padding\n",
    " in_channels 表示的是输入卷积层的图片的厚度\n",
    " out_channels 表示的是要输出的厚度\n",
    " kernel_size 表示的是卷积核的大小，可以用一个数字表示长宽相等的卷积核，比如 kernel_size=3 ,也可以用不同的数字表示长宽不同的卷积核，比如 kernel_size=(3,2) \n",
    " stride 表示卷积核滑动的步长\n",
    " padding 表示的是图片周围填充 0 的多少，padding=0 表示不填充，padding=1 表示四周都填充 1维\n",
    " - 3、nn.ReLU()\n",
    " 这个表示使用 ReLU() 激活函数，里面有一个参数 inplace，默认设置为 False，表示新创建一个对象对其修改，也可以设置为 True，表示直接对这个对象进行修改。\n",
    " - 4、nn.MaxPool2d()\n",
    " 这个是最大池化层，当然也有平均池化层，里面的参数有 kernel_size，stride，padding\n",
    " kernel_size 表示池化的窗口大小，和卷积里面的 kernel_size 是一样的\n",
    " stride 也和卷积层里面一样，需要自己设置滑动步长\n",
    " padding也和卷积层里面的参数是一样的，默认是 0\n",
    " 模型需要传入的参数是输入的图片维数以及输出的种类数\n",
    " \n",
    "### 2.3、训练\n",
    "\n",
    "训练过程是一样的，只是输入的图片不再需要展开\n",
    "\n",
    "这是训练 20 个 epoch 的结果，当然你也可以增加训练次数，修改里面的参数达到更好的效果，可以参考一下 LeNet 的网络结构，自己重新写一下\n",
    "\n",
    "![](img/pt_4_1.jpg)\n",
    "\n",
    "大体上简单的卷积神经网络就是这么构建的，当然现在也有很多复杂的网络，比如 vgg，inceptionv1-v4，resnet 以及修正的 inception-resnet，这些网络都是深层的卷积网络，有兴趣的同学可以去看看 pytorch 的官方代码实现，或者去 github 上搜索相应的网络。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
